{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMA+2LF6K7WCK6drjVqnjuu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cnovak232/DL_Speech_Enhancement/blob/main/DL_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data by cloning the repo - easiest way to access shared data"
      ],
      "metadata": {
        "id": "kA2ws-l-yuS9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9xENf0M-0vZ"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/cnovak232/DL_Speech_Enhancement.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyWavelets"
      ],
      "metadata": {
        "id": "nm4bwTie7Xvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define some helper function for plotting and playing audio"
      ],
      "metadata": {
        "id": "Q1RLKW7oy2Yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio as ta\n",
        "import librosa as lib\n",
        "from IPython.display import Audio, display\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pywt\n",
        "\n",
        "# helper functions for audio and what not\n",
        "# mostly taken for torchaudio tutorials \n",
        "\n",
        "def play_audio(waveform, sample_rate):\n",
        "    waveform = waveform.numpy()\n",
        "\n",
        "    num_channels, num_frames = waveform.shape\n",
        "    if num_channels == 1:\n",
        "        display(Audio(waveform[0], rate=sample_rate))\n",
        "    elif num_channels == 2:\n",
        "        display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n",
        "    else:\n",
        "        raise ValueError(\"Waveform with more than 2 channels are not supported.\")\n",
        "\n",
        "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
        "    waveform = waveform.numpy()\n",
        "\n",
        "    num_channels, num_frames = waveform.shape\n",
        "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
        "\n",
        "    figure, axes = plt.subplots(num_channels, 1)\n",
        "    if num_channels == 1:\n",
        "        axes = [axes]\n",
        "    for c in range(num_channels):\n",
        "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
        "        axes[c].grid(True)\n",
        "    if num_channels > 1:\n",
        "        axes[c].set_ylabel(f'Channel {c+1}')\n",
        "    if xlim:\n",
        "        axes[c].set_xlim(xlim)\n",
        "    if ylim:\n",
        "        axes[c].set_ylim(ylim)\n",
        "\n",
        "    figure.suptitle(title)\n",
        "    plt.show(block=False)\n",
        "\n",
        "def get_spectrogram(\n",
        "    waveform = None,\n",
        "    n_fft = 512,\n",
        "    win_len = None,\n",
        "    hop_len = None,\n",
        "    power = 1.0 ):\n",
        "    spectrogram = ta.transforms.Spectrogram(\n",
        "      n_fft=n_fft,\n",
        "      win_length=win_len,\n",
        "      hop_length=hop_len,\n",
        "      center=True,\n",
        "      pad_mode=\"reflect\",\n",
        "      power=power )\n",
        "    \n",
        "    return spectrogram(waveform)\n",
        "\n",
        "def plot_spectrogram(spec, type = \"amplitude\", title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
        "    fig, axs = plt.subplots(1, 1)\n",
        "    axs.set_title(title or 'Spectrogram (db)')\n",
        "    axs.set_ylabel(ylabel)\n",
        "    axs.set_xlabel('frame')\n",
        "    toDb = ta.transforms.AmplitudeToDB(type)\n",
        "    im = axs.imshow(toDb(spec), origin='lower', aspect=aspect)\n",
        "    if xmax:\n",
        "        axs.set_xlim((0, xmax))\n",
        "    fig.colorbar(im, ax=axs)\n",
        "    plt.show(block=False)\n",
        "\n",
        "\n",
        "class DWT(object):\n",
        "    def __init__(self, name, level):\n",
        "        self.name = name\n",
        "        self.level = level\n",
        "\n",
        "    def __call__(self, x):\n",
        "        coeffs = pywt.wavedec(x, self.name, level=self.level,mode='per')\n",
        "        xdwt = torch.empty(0)\n",
        "        for band in coeffs:\n",
        "            torch.from_numpy(band)\n",
        "            xdwt = torch.cat( (xdwt, torch.from_numpy(band)), dim=1)\n",
        "        return xdwt\n",
        "\n",
        "class IDWT(object):\n",
        "    def __init__(self, name, level):\n",
        "        self.name = name\n",
        "        self.level = level\n",
        "\n",
        "    def __call__(self, xdwt):\n",
        "        length = xdwt.shape[1]\n",
        "        coeffs = []\n",
        "        for idx in range(self.level):\n",
        "            hlen = length // 2\n",
        "            coeffs.append( xdwt[:,hlen:length].numpy() )\n",
        "            length = hlen\n",
        "\n",
        "        coeffs.append( xdwt[:,:length].numpy() )\n",
        "        coeffs.reverse()\n",
        "\n",
        "        xr = pywt.waverec(coeffs, self.name, mode='per')\n",
        "        return torch.from_numpy(xr) \n",
        "\n",
        "def norm_spec( spec ):\n",
        "    return spec, 1\n",
        "    #normed = spec / spec.max()\n",
        "    #return normed, spec.max()\n"
      ],
      "metadata": {
        "id": "re5VYgn7UP0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a Custom Dataset class for the Data and read it in"
      ],
      "metadata": {
        "id": "ryD_nLFqy7zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read dataset in and downsample / transform / pad if needed\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import os\n",
        "\n",
        "class VoiceBankDemand(Dataset):\n",
        "    def __init__(self, clean_dir, noisy_dir, list_dir, \n",
        "                 data = \"train\", len_samples = None, downsample = None, \n",
        "                 transform = None ):\n",
        "        self.clean_dir = clean_dir\n",
        "        self.noisy_dir = noisy_dir\n",
        "        self.list_dir = list_dir\n",
        "        self.num_samples = len_samples\n",
        "        self.downsample = downsample\n",
        "        self.transform = transform\n",
        "        self.data = data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.list_dir)\n",
        "\n",
        "    def __getitem__( self, idx ):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        clean_name = os.path.join( self.clean_dir, self.list_dir[idx] )\n",
        "        noisy_name = os.path.join( self.noisy_dir, self.list_dir[idx] )\n",
        "        clean_audio, fs = ta.load(clean_name)\n",
        "        noisy_audio, fs= ta.load(noisy_name)\n",
        "\n",
        "        if self.downsample:\n",
        "            downsampler = ta.transforms.Resample(fs,self.downsample)\n",
        "            clean_audio = downsampler( clean_audio )\n",
        "            noisy_audio = downsampler( noisy_audio )\n",
        "\n",
        "        orig_len = clean_audio.shape[1]\n",
        "\n",
        "        if self.num_samples:\n",
        "            # trim or pad audio to fixed length \n",
        "            if clean_audio.shape[1] > num_samples:\n",
        "                clean_audio = clean_audio[:,:num_samples]\n",
        "                noisy_audio = noisy_audio[:,:num_samples]\n",
        "            elif clean_audio.shape[1] < num_samples:\n",
        "                pad_len = int( num_samples - clean_audio.shape[1] )\n",
        "                pad = torch.zeros(1,pad_len)\n",
        "                clean_audio = torch.cat((clean_audio,pad), dim=1)\n",
        "                noisy_audio = torch.cat((noisy_audio,pad),dim=1)\n",
        "        \n",
        "        if self.data == \"test\":\n",
        "            if self.transform:\n",
        "                noisy_audio = self.transform( noisy_audio )\n",
        "                clean_audio = self.transform( clean_audio )\n",
        "                #clean_mag,_ = torch.abs(clean_trnsfrm) \n",
        "                #clean_audio = (clean_audio, clean_mag)\n",
        "               # noisy_mag, norm_val = torch.abs(noisy_trnsfrm)\n",
        "                #noisy_phase = torch.angle(noisy_trnsfrm)\n",
        "                #noisy_audio = (noisy_mag, noisy_phase, noisy_audio, norm_val)\n",
        "        else:\n",
        "            if self.transform:\n",
        "                clean_audio  = self.transform( clean_audio )\n",
        "                noisy_audio  = self.transform( noisy_audio ) \n",
        "\n",
        "        sample = (clean_audio, noisy_audio, orig_len)\n",
        "\n",
        "        return sample\n",
        "\n",
        "train_clean_path = './DL_Speech_Enhancement/clean_trainset_28spk_wav'\n",
        "train_noisy_path = './DL_Speech_Enhancement/noisy_trainset_28spk_wav'\n",
        "test_clean_path  = './DL_Speech_Enhancement/clean_testset_wav'\n",
        "test_noisy_path  = './DL_Speech_Enhancement/noisy_testset_wav'\n",
        "\n",
        "list_dir_train = os.listdir(train_clean_path)\n",
        "list_dir_train.sort()\n",
        "list_dir_test = os.listdir(test_clean_path)\n",
        "list_dir_test.sort()\n",
        "\n",
        "target_fs = 8000 # downsample to 16 KHz\n",
        "spectrogram = ta.transforms.Spectrogram(\n",
        "    n_fft=320, # size for the CRNN\n",
        "    power=1.0,\n",
        "    normalized = False )\n",
        "\n",
        "WAVE_UNET_LEN = 16384 # length the WaveUNET processing\n",
        "num_samples = WAVE_UNET_LEN * 3\n",
        "\n",
        "\n",
        "wavelet = DWT( name = 'db8', level = 5 )\n",
        "\n",
        "\n",
        "# return turns a tuple of ( clean_data,noisy_data,original_length)\n",
        "# data type depends on if transform was specified\n",
        "train_set = VoiceBankDemand( clean_dir = train_clean_path,\n",
        "                             noisy_dir = train_noisy_path,\n",
        "                             list_dir = list_dir_train,\n",
        "                             len_samples = None, # clip or pad samples to be 5s\n",
        "                             downsample = target_fs, # downsample to 16Khz\n",
        "                             transform = spectrogram )\n",
        "\n",
        "# returns tuples within tuples: clean_data, noisy_data, original length\n",
        "# If using a transform (like spectrogram) the subtuples will be:\n",
        "# clean_data: (clean_waveform, clean_spec)\n",
        "# noisy_data: ( noisy_spec, noisy_phase, noisy_waveform, normalization_val )\n",
        "test_set = VoiceBankDemand( clean_dir = test_clean_path,\n",
        "                            noisy_dir = test_noisy_path,\n",
        "                            list_dir = list_dir_test,\n",
        "                            data = \"test\",\n",
        "                            len_samples = None,\n",
        "                            downsample = target_fs,\n",
        "                            transform = spectrogram )\n",
        "\n",
        "#############################################################\n",
        "# for viewing samples and such\n",
        "clean, noisey, orgin_len = train_set[1]\n",
        "\n",
        "#transform = DWT( name = 'db4', level = 3 )\n",
        "#inv_transform = IDWT( name = 'db4', level = 3)\n",
        "#clean_dwt = transform(clean)\n",
        "\n",
        "\n",
        "#cleanr = inv_transform(clean_dwt)\n",
        "\n",
        "#plot_waveform(clean_dwt,target_fs)\n",
        "#plot_waveform(cleanr,target_fs)\n",
        "\n",
        "clean_test, noisy_test, orig_len = test_set[0]\n",
        "\n",
        "#noisy_mag, noisy_phase, noisy_audio, norm_val = noisy_test\n",
        "\n",
        "plot_spectrogram(clean.squeeze())\n",
        "plot_spectrogram(noisey.squeeze())\n",
        "\n",
        "#play_audio(noisy_audio[:,:orig_len],target_fs)"
      ],
      "metadata": {
        "id": "iYwWekZk-4kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DOESN\"T WORK WELL, DO NOT USE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# U-Net Style Autoencoder from to start from\n",
        "# https://medium.com/@sriskandaryan/autoencoders-demystified-audio-signal-denoising-32a491ab023a\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, chnls_in=1, chnls_out=1):\n",
        "        super(UNet, self).__init__()\n",
        "        self.down_conv_layer_1 = DownConvBlock(chnls_in, 64, norm=False)\n",
        "        self.down_conv_layer_2 = DownConvBlock(64, 128)\n",
        "        self.down_conv_layer_3 = DownConvBlock(128, 256)\n",
        "        self.down_conv_layer_4 = DownConvBlock(256, 256, dropout=0.5)\n",
        "        self.down_conv_layer_5 = DownConvBlock(256, 256, dropout=0.5)\n",
        "        self.down_conv_layer_6 = DownConvBlock(256, 256, dropout=0.5)\n",
        "\n",
        "        self.up_conv_layer_1 = UpConvBlock(256, 256, kernel_size=(2,3), stride=2, padding=0, dropout=0.5)# 256+256 6 5 kernel_size=(2, 3), stride=2, padding=0\n",
        "        self.up_conv_layer_2 = UpConvBlock(512, 256, kernel_size=(2,3), stride=2, padding=0, dropout=0.5) # 256+256 1 4\n",
        "        self.up_conv_layer_3 = UpConvBlock(512, 256, kernel_size=(2,3), stride=2, padding=0, output_padding = 2, dropout=0.5) # 2 3\n",
        "        self.up_conv_layer_4 = UpConvBlock(512, 128, dropout=0.5) # 3 2\n",
        "        self.up_conv_layer_5 = UpConvBlock(256, 64) # 4 1\n",
        "        self.up_conv_layer_6 = UpConvBlock(512, 128)\n",
        "        self.up_conv_layer_7 = UpConvBlock(256, 64)\n",
        "        self.upsample_layer = nn.Upsample(scale_factor=2)\n",
        "        self.zero_pad = nn.ZeroPad2d((1, 0, 1, 0))\n",
        "        self.conv_layer_1 = nn.Conv2d(128, chnls_out, 4, padding='same')\n",
        "        self.activation = nn.Tanh()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        enc1 = self.down_conv_layer_1(x)\n",
        "        enc2 = self.down_conv_layer_2(enc1) \n",
        "        enc3 = self.down_conv_layer_3(enc2)\n",
        "        enc4 = self.down_conv_layer_4(enc3)\n",
        "        enc5 = self.down_conv_layer_5(enc4)\n",
        "        enc6 = self.down_conv_layer_6(enc5)\n",
        " \n",
        "        dec1 = self.up_conv_layer_1(enc6, enc5)\n",
        "        dec2 = self.up_conv_layer_2(dec1, enc4)\n",
        "        dec3 = self.up_conv_layer_3(dec2, enc3)\n",
        "        dec4 = self.up_conv_layer_4(dec3, enc2)\n",
        "        dec5 = self.up_conv_layer_5(dec4, enc1)\n",
        "\n",
        "        final = self.upsample_layer(dec5)\n",
        "        final = self.zero_pad(final)\n",
        "        final = self.conv_layer_1(final)\n",
        "        return final\n",
        "\n",
        "class UpConvBlock(nn.Module):\n",
        "    def __init__(self, ip_sz, op_sz, kernel_size=4, stride= 2, padding=0 , output_padding = 0, dropout=0.0):\n",
        "        super(UpConvBlock, self).__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.ConvTranspose2d(ip_sz, op_sz, kernel_size=kernel_size, stride=stride, padding=padding, output_padding = output_padding),\n",
        "            nn.InstanceNorm2d(op_sz),\n",
        "            nn.ReLU(),\n",
        "        ])\n",
        "        if dropout:\n",
        "            self.layers += [nn.Dropout(dropout)]\n",
        "    def forward(self, x, enc_ip):\n",
        "        x = nn.Sequential(*(self.layers))(x)\n",
        "        op = torch.cat((x, enc_ip), 1)\n",
        "        return op\n",
        "\n",
        "\n",
        "class DownConvBlock(nn.Module):\n",
        "    def __init__(self, ip_sz, op_sz, kernel_size=4, norm=True, dropout=0.0):\n",
        "        super(DownConvBlock, self).__init__()\n",
        "        self.layers = nn.ModuleList([nn.Conv2d(ip_sz, op_sz, kernel_size, 2, 1)])\n",
        "        if norm:\n",
        "            self.layers.append(nn.InstanceNorm2d(op_sz))\n",
        "        self.layers += [nn.LeakyReLU(0.2)]\n",
        "        if dropout:\n",
        "            self.layers += [nn.Dropout(dropout)]\n",
        "    def forward(self, x):\n",
        "        op = nn.Sequential(*(self.layers))(x)\n",
        "        return op"
      ],
      "metadata": {
        "id": "3dwSrp3PVAcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.utils.rnn as rnn\n",
        "\n",
        "class SpeechEnhancementLSTM(nn.Module):\n",
        "  #input size, hidden size, number of layers, output size, and a flag for using bidirectional LSTMs as its parameters\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, bidirectional=True):\n",
        "        super(SpeechEnhancementLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
        "        self.fc = nn.Linear(hidden_size * 2 if bidirectional else hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        x = rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "        x, _ = self.lstm(x)\n",
        "        x, _ = rnn.pad_packed_sequence(x, batch_first=True)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "EKx1nsuT7q76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EHNET or CRN: Convoluation Encoder/Decoder with LSTM in the middle\n",
        "Processing spectrogram inputs \n",
        "\n",
        "Paper: \n",
        "https://web.cse.ohio-state.edu/~wang.77/papers/Tan-Wang1.interspeech18.pdf\n",
        "\n",
        "Code Reference:\n",
        "https://github.com/haoxiangsnr/A-Convolutional-Recurrent-Neural-Network-for-Real-Time-Speech-Enhancement/blob/master/model/crn_in_paper_layer_norm.py"
      ],
      "metadata": {
        "id": "b3jxIDPv-_j9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class CausalConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=(3, 2),\n",
        "            stride=(2, 1),\n",
        "            padding=(0, 1)\n",
        "        )\n",
        "        self.norm = nn.BatchNorm2d(num_features=out_channels)\n",
        "        self.activation = nn.ELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        2D Causal convolution.\n",
        "        Args:\n",
        "            x: [B, C, F, T]\n",
        "        Returns:\n",
        "            [B, C, F, T]\n",
        "        \"\"\"\n",
        "        x = self.conv(x)\n",
        "        x = x[:, :, :, :-1]  # chomp size\n",
        "        x = self.norm(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CausalTransConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, is_last=False, output_padding=(0, 0)):\n",
        "        super().__init__()\n",
        "        self.conv = nn.ConvTranspose2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=(3, 2),\n",
        "            stride=(2, 1),\n",
        "            output_padding=output_padding\n",
        "        )\n",
        "        self.norm = nn.BatchNorm2d(num_features=out_channels)\n",
        "        if is_last:\n",
        "            self.activation = nn.ReLU()\n",
        "        else:\n",
        "            self.activation = nn.ELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        2D Causal convolution.\n",
        "        Args:\n",
        "            x: [B, C, F, T]\n",
        "        Returns:\n",
        "            [B, C, F, T]\n",
        "        \"\"\"\n",
        "        x = self.conv(x)\n",
        "        x = x[:, :, :, :-1]  # chomp size\n",
        "        x = self.norm(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CRN(nn.Module):\n",
        "    \"\"\"\n",
        "    Input: [batch size, channels=1, T, n_fft]\n",
        "    Output: [batch size, T, n_fft]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CRN, self).__init__()\n",
        "        # Encoder\n",
        "        self.conv_block_1 = CausalConvBlock(1, 16)\n",
        "        self.conv_block_2 = CausalConvBlock(16, 32)\n",
        "        self.conv_block_3 = CausalConvBlock(32, 64)\n",
        "        self.conv_block_4 = CausalConvBlock(64, 128)\n",
        "        self.conv_block_5 = CausalConvBlock(128, 256)\n",
        "\n",
        "        # LSTM\n",
        "        self.lstm_layer = nn.LSTM(input_size=1024, hidden_size=1024, num_layers=2, batch_first=True)\n",
        "\n",
        "        self.tran_conv_block_1 = CausalTransConvBlock(256 + 256, 128)\n",
        "        self.tran_conv_block_2 = CausalTransConvBlock(128 + 128, 64)\n",
        "        self.tran_conv_block_3 = CausalTransConvBlock(64 + 64, 32)\n",
        "        self.tran_conv_block_4 = CausalTransConvBlock(32 + 32, 16, output_padding=(1, 0))\n",
        "        self.tran_conv_block_5 = CausalTransConvBlock(16 + 16, 1, is_last=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.lstm_layer.flatten_parameters()\n",
        "\n",
        "        e_1 = self.conv_block_1(x)\n",
        "        e_2 = self.conv_block_2(e_1)\n",
        "        e_3 = self.conv_block_3(e_2)\n",
        "        e_4 = self.conv_block_4(e_3)\n",
        "        e_5 = self.conv_block_5(e_4)  # [2, 256, 4, 200]\n",
        "\n",
        "        batch_size, n_channels, n_f_bins, n_frame_size = e_5.shape\n",
        "\n",
        "        # [2, 256, 4, 200] = [2, 1024, 200] => [2, 200, 1024]\n",
        "        lstm_in = e_5.reshape(batch_size, n_channels * n_f_bins, n_frame_size).permute(0, 2, 1)\n",
        "        lstm_out, _ = self.lstm_layer(lstm_in)  # [2, 200, 1024]\n",
        "        lstm_out = lstm_out.permute(0, 2, 1).reshape(batch_size, n_channels, n_f_bins, n_frame_size)  # [2, 256, 4, 200]\n",
        "\n",
        "        d_1 = self.tran_conv_block_1(torch.cat((lstm_out, e_5), 1))\n",
        "        d_2 = self.tran_conv_block_2(torch.cat((d_1, e_4), 1))\n",
        "        d_3 = self.tran_conv_block_3(torch.cat((d_2, e_3), 1))\n",
        "        d_4 = self.tran_conv_block_4(torch.cat((d_3, e_2), 1))\n",
        "        d_5 = self.tran_conv_block_5(torch.cat((d_4, e_1), 1))\n",
        "\n",
        "        return d_5"
      ],
      "metadata": {
        "id": "aUTC3kpH-g2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WaveU-Net for Speech Denoising\n",
        "\n",
        "Code Reference:\n",
        "https://github.com/haoxiangsnr/Wave-U-Net-for-Speech-Enhancement\n",
        "\n",
        "Paper Reference:\n",
        "https://arxiv.org/pdf/1806.03185.pdf\n"
      ],
      "metadata": {
        "id": "cAqjnlo03ig6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DownSamplingLayer(nn.Module):\n",
        "    def __init__(self, channel_in, channel_out, dilation=1, kernel_size=15, stride=1, padding=7):\n",
        "        super(DownSamplingLayer, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv1d(channel_in, channel_out, kernel_size=kernel_size,\n",
        "                      stride=stride, padding=padding, dilation=dilation),\n",
        "            nn.BatchNorm1d(channel_out),\n",
        "            nn.LeakyReLU(negative_slope=0.1)\n",
        "        )\n",
        "\n",
        "    def forward(self, ipt):\n",
        "        return self.main(ipt)\n",
        "\n",
        "class UpSamplingLayer(nn.Module):\n",
        "    def __init__(self, channel_in, channel_out, kernel_size=5, stride=1, padding=2):\n",
        "        super(UpSamplingLayer, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv1d(channel_in, channel_out, kernel_size=kernel_size,\n",
        "                      stride=stride, padding=padding),\n",
        "            nn.BatchNorm1d(channel_out),\n",
        "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, ipt):\n",
        "        return self.main(ipt)\n",
        "\n",
        "class WaveUNet2(nn.Module):\n",
        "    def __init__(self, n_layers=12, channels_interval=24):\n",
        "        super(WaveUNet2, self).__init__()\n",
        "\n",
        "        self.n_layers = n_layers\n",
        "        self.channels_interval = channels_interval\n",
        "        encoder_in_channels_list = [1] + [i * self.channels_interval for i in range(1, self.n_layers)]\n",
        "        encoder_out_channels_list = [i * self.channels_interval for i in range(1, self.n_layers + 1)]\n",
        "\n",
        "        #          1    => 2    => 3    => 4    => 5    => 6   => 7   => 8   => 9  => 10 => 11 =>12\n",
        "        # 16384 => 8192 => 4096 => 2048 => 1024 => 512 => 256 => 128 => 64 => 32 => 16 =>  8 => 4\n",
        "        self.encoder = nn.ModuleList()\n",
        "        for i in range(self.n_layers):\n",
        "            self.encoder.append(\n",
        "                DownSamplingLayer(\n",
        "                    channel_in=encoder_in_channels_list[i],\n",
        "                    channel_out=encoder_out_channels_list[i]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.middle = nn.Sequential(\n",
        "            nn.Conv1d(self.n_layers * self.channels_interval, self.n_layers * self.channels_interval, 15, stride=1,\n",
        "                      padding=7),\n",
        "            nn.BatchNorm1d(self.n_layers * self.channels_interval),\n",
        "            nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        )\n",
        "\n",
        "        decoder_in_channels_list = [(2 * i + 1) * self.channels_interval for i in range(1, self.n_layers)] + [\n",
        "            2 * self.n_layers * self.channels_interval]\n",
        "        decoder_in_channels_list = decoder_in_channels_list[::-1]\n",
        "        decoder_out_channels_list = encoder_out_channels_list[::-1]\n",
        "        self.decoder = nn.ModuleList()\n",
        "        for i in range(self.n_layers):\n",
        "            self.decoder.append(\n",
        "                UpSamplingLayer(\n",
        "                    channel_in=decoder_in_channels_list[i],\n",
        "                    channel_out=decoder_out_channels_list[i]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Conv1d(1 + self.channels_interval, 1, kernel_size=1, stride=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        tmp = []\n",
        "        o = input\n",
        "\n",
        "        # Up Sampling\n",
        "        for i in range(self.n_layers):\n",
        "            o = self.encoder[i](o)\n",
        "            tmp.append(o)\n",
        "            # [batch_size, T // 2, channels]\n",
        "            o = o[:, :, ::2]\n",
        "\n",
        "        o = self.middle(o)\n",
        "\n",
        "        # Down Sampling\n",
        "        for i in range(self.n_layers):\n",
        "            # [batch_size, T * 2, channels]\n",
        "            o = F.interpolate(o, scale_factor=2, mode=\"linear\", align_corners=True)\n",
        "            # Skip Connection\n",
        "            o = torch.cat([o, tmp[self.n_layers - i - 1]], dim=1)\n",
        "            o = self.decoder[i](o)\n",
        "\n",
        "        o = torch.cat([o, input], dim=1)\n",
        "        o = self.out(o)\n",
        "        return o"
      ],
      "metadata": {
        "id": "WLqMI3IAmWoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main block for training the model and testing.\n",
        "Initializes all training and testing parameters. \n",
        "Performs training and testing in one big loop, testing each epoch to track any overfitting."
      ],
      "metadata": {
        "id": "3DhbW9p3yyaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training (work in progress)\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_model( model, dataloader, criterion, optimizer, device ):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Iterate over data.\n",
        "    for clean_audio,noisy_audio,orig_len in dataloader:\n",
        "\n",
        "        # send inputs to gpu\n",
        "        clean_audio = clean_audio.to(device)\n",
        "        noisy_audio = noisy_audio.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        with torch.enable_grad():\n",
        "            # Send the noisey speech sample through network\n",
        "            output = model(noisy_audio)\n",
        "\n",
        "            # compute loss between network output and clean audio\n",
        "            loss = criterion(output, clean_audio)\n",
        "\n",
        "            # backward + optimize \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "\n",
        "    return model, epoch_loss\n",
        "\n",
        "def test_model( model, device, criterion, dataloader, inv_transform = None ):\n",
        "    model.eval() \n",
        "\n",
        "    #cleaned_data = torch.empty(0)\n",
        "    cleaned_data = []\n",
        "    running_loss = 0\n",
        "\n",
        "    for clean_audio, noisy_audio, orig_len in dataloader:\n",
        "        #noisy_mag, noisy_phase, noisy_wav, norm_val = noisy_audio\n",
        "        noisy_audio = noisy_audio.to(device)\n",
        "       # clean_wav, clean_mag = clean_audio\n",
        "        clean_audio = clean_audio.to(device)\n",
        "        #norm_val = norm_val.to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # forward\n",
        "            enhanced_data = model(noisy_audio)\n",
        "\n",
        "            loss = criterion(enhanced_data, clean_audio)\n",
        "\n",
        "            #cleaned_data = torch.cat( (cleaned_data, enhanced_data.detach().cpu()), 0 )\n",
        "            cleaned_data.append( enhanced_data.detach().cpu() )\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    test_loss = running_loss / len( dataloader.dataset )\n",
        "\n",
        "    return cleaned_data, test_loss\n",
        "\n",
        "\n",
        "train_loader = DataLoader( train_set, batch_size=1)\n",
        "test_loader = DataLoader( test_set, batch_size=1 )\n",
        "\n",
        "model = CRN() # assign model you want to use\n",
        "\n",
        "learning_rate = 0.01\n",
        "num_epochs = 20\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(),learning_rate)\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "prev_test_loss = 10000.0\n",
        "count = 0\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "    print('-' * 10)\n",
        "\n",
        "    model, epoch_loss = train_model( model, train_loader, criterion, optimizer, device )\n",
        "    print('Train Epoch Loss = ', epoch_loss )\n",
        "\n",
        "    cleaned_data, test_loss = \\\n",
        "        test_model( model, device, criterion, test_loader, inv_transform = None)\n",
        "    print('Test Epoch Loss = ', test_loss )\n",
        "\n",
        "    if prev_test_loss < test_loss:\n",
        "        count += 1\n",
        "    else:\n",
        "        count = 0\n",
        "\n",
        "    if count > 1:\n",
        "        break;\n",
        "\n",
        "    prev_test_loss = test_loss\n"
      ],
      "metadata": {
        "id": "wq5oFQsLVP4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics[audio]\n",
        "!pip install pesq"
      ],
      "metadata": {
        "id": "6IBSOg47iL9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loop through testing samples and evaluate enhanced data against clean data"
      ],
      "metadata": {
        "id": "XCimwEXBZzOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import SignalNoiseRatio\n",
        "from pesq import pesq\n",
        "\n",
        "SNR = SignalNoiseRatio(zero_mean=True)\n",
        "\n",
        "snr_vals_noisy = torch.empty(len(test_set))\n",
        "pesq_vals_noisy = torch.empty(len(test_set))\n",
        "snr_vals = torch.empty(len(test_set))\n",
        "pesq_vals = torch.empty(len(test_set))\n",
        "cleaned_waveforms = []\n",
        "#inv_transform = IDWT( name = 'db8', level = 5)\n",
        "inv_transform = ta.transforms.InverseSpectrogram( \n",
        "    n_fft = 321,\n",
        "    normalized = False)\n",
        "\n",
        "complex_spec = ta.transforms.Spectrogram(\n",
        "    n_fft=321,\n",
        "    power=None,\n",
        "    normalized = False ) # return complex spectrum\n",
        "\n",
        "test_complex = VoiceBankDemand( clean_dir = test_clean_path,\n",
        "                             noisy_dir = test_noisy_path,\n",
        "                             list_dir = list_dir_test,\n",
        "                             data = \"test\",\n",
        "                             len_samples = None,\n",
        "                             downsample = target_fs,\n",
        "                             transform = complex_spec )\n",
        "\n",
        "test_wavs = VoiceBankDemand( clean_dir = test_clean_path,\n",
        "                             noisy_dir = test_noisy_path,\n",
        "                             list_dir = list_dir_test,\n",
        "                             data = \"test\",\n",
        "                             len_samples = None,\n",
        "                             downsample = target_fs,\n",
        "                             transform = None )\n",
        "\n",
        "for i in range( len(test_wavs) ):\n",
        "    clean_wav,noisy_wav,orig_len = test_wavs[i]\n",
        "    enhanced_audio = cleaned_data[i]\n",
        "\n",
        "    enhanced_audio = enhanced_audio.detach().cpu()\n",
        "\n",
        "    if inv_transform:\n",
        "        _,noisy_complex, orig_len = test_complex[i]\n",
        "        phase = torch.angle(noisy_complex)\n",
        "        complex_audio = torch.polar(enhanced_audio.squeeze(), phase)\n",
        "        enhanced_audio = inv_transform( complex_audio, clean_wav.shape[1] )\n",
        "\n",
        "    if enhanced_audio.shape[1] > orig_len:\n",
        "        enhanced_audio = enhanced_audio[:,:orig_len]\n",
        "        noisy_wav = noisy_wav[:,:orig_len]\n",
        "        clean_wav = clean_wav[:,:orig_len]\n",
        "\n",
        "    cleaned_waveforms.append( enhanced_audio )\n",
        "\n",
        "    snr_vals[i] = SNR(enhanced_audio, clean_wav)\n",
        "    pesq_vals[i] = pesq( target_fs, enhanced_audio.squeeze().numpy(), clean_wav.squeeze().numpy(), \"nb\")\n",
        "\n",
        "    snr_vals_noisy[i] = SNR(noisy_wav, clean_wav)\n",
        "    pesq_vals_noisy[i] = pesq( target_fs, noisy_wav.squeeze().numpy(), clean_wav.squeeze().numpy(), \"nb\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xUkP-YBVe1_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute average and best SNR and PESQ improvements"
      ],
      "metadata": {
        "id": "cyg1ZFIkZQKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SNR_imp = snr_vals - snr_vals_noisy\n",
        "PESQ_imp = pesq_vals - pesq_vals_noisy\n",
        "print(\"Average PESQ Enhanced\", pesq_vals.sum() / len(pesq_vals) )\n",
        "print(\"Average PESQ Noisy\", pesq_vals_noisy.sum() / len(pesq_vals_noisy) )\n",
        "\n",
        "avg_SNR_imp = SNR_imp.sum() / len(SNR_imp)\n",
        "avg_PESQ_imp = PESQ_imp.sum() / len(PESQ_imp)\n",
        "\n",
        "peak_SNR_imp = SNR_imp.max()\n",
        "peak_snr_ind = SNR_imp.argmax()\n",
        "peak_PESQ_imp = PESQ_imp.max()\n",
        "peak_PESQ_ind = PESQ_imp.argmax()\n",
        "\n",
        "\n",
        "print(\"Average SNR Improvement = \", avg_SNR_imp )\n",
        "print(\"Average PESQ Improvement = \", avg_PESQ_imp )\n",
        "\n",
        "print(\"Best SNR Improvement = \", peak_SNR_imp )\n",
        "print(\"Best PESQ Improvment = \", peak_PESQ_imp )\n",
        "\n",
        "\n",
        "# use if using magnitude spectrograms\n",
        "#plot_spectrogram(cleaned_data[0].squeeze())\n",
        "#plot_spectrogram(test_set[0][0][1].squeeze())\n",
        "\n",
        "play_audio(test_wavs[800][1],target_fs) # play noisy test sample\n",
        "play_audio(cleaned_waveforms[800],target_fs)\n"
      ],
      "metadata": {
        "id": "cIfbEp1hjocU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print( snr_vals_noisy.argmin() )\n",
        "print(pesq_vals_noisy[588] )"
      ],
      "metadata": {
        "id": "8w8m-6dxAJUG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}