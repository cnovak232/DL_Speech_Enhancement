{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2+Mw350wBFwS1v+wzuk4P"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A9xENf0M-0vZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d8f885e-5f99-4685-beb4-6c17aa2d9b8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DL_Speech_Enhancement'...\n",
            "remote: Enumerating objects: 8284, done.\u001b[K\n",
            "remote: Counting objects: 100% (1664/1664), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1659/1659), done.\u001b[K\n",
            "remote: Total 8284 (delta 4), reused 1664 (delta 4), pack-reused 6620\u001b[K\n",
            "Receiving objects: 100% (8284/8284), 1.70 GiB | 23.12 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n",
            "Updating files: 100% (8255/8255), done.\n"
          ]
        }
      ],
      "source": [
        "# easiest way to load the data in to google colab from central location\n",
        "!git clone https://github.com/cnovak232/DL_Speech_Enhancement.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio as ta\n",
        "import librosa as lib\n",
        "from IPython.display import Audio, display\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# helper functions for audio and what not\n",
        "# mostly taken for torchaudio tutorials \n",
        "\n",
        "def play_audio(waveform, sample_rate):\n",
        "    waveform = waveform.numpy()\n",
        "\n",
        "    num_channels, num_frames = waveform.shape\n",
        "    if num_channels == 1:\n",
        "        display(Audio(waveform[0], rate=sample_rate))\n",
        "    elif num_channels == 2:\n",
        "        display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n",
        "    else:\n",
        "        raise ValueError(\"Waveform with more than 2 channels are not supported.\")\n",
        "\n",
        "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
        "    waveform = waveform.numpy()\n",
        "\n",
        "    num_channels, num_frames = waveform.shape\n",
        "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
        "\n",
        "    figure, axes = plt.subplots(num_channels, 1)\n",
        "    if num_channels == 1:\n",
        "        axes = [axes]\n",
        "    for c in range(num_channels):\n",
        "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
        "        axes[c].grid(True)\n",
        "    if num_channels > 1:\n",
        "        axes[c].set_ylabel(f'Channel {c+1}')\n",
        "    if xlim:\n",
        "        axes[c].set_xlim(xlim)\n",
        "    if ylim:\n",
        "        axes[c].set_ylim(ylim)\n",
        "\n",
        "    figure.suptitle(title)\n",
        "    plt.show(block=False)\n",
        "\n",
        "def get_spectrogram(\n",
        "    waveform = None,\n",
        "    n_fft = 512,\n",
        "    win_len = None,\n",
        "    hop_len = None,\n",
        "    power = 1.0 ):\n",
        "    spectrogram = ta.transforms.Spectrogram(\n",
        "      n_fft=n_fft,\n",
        "      win_length=win_len,\n",
        "      hop_length=hop_len,\n",
        "      center=True,\n",
        "      pad_mode=\"reflect\",\n",
        "      power=power )\n",
        "    \n",
        "    return spectrogram(waveform)\n",
        "\n",
        "def plot_spectrogram(spec, type = \"amplitude\", title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
        "    fig, axs = plt.subplots(1, 1)\n",
        "    axs.set_title(title or 'Spectrogram (db)')\n",
        "    axs.set_ylabel(ylabel)\n",
        "    axs.set_xlabel('frame')\n",
        "    toDb = ta.transforms.AmplitudeToDB(type)\n",
        "    im = axs.imshow(toDb(spec), origin='lower', aspect=aspect)\n",
        "    if xmax:\n",
        "        axs.set_xlim((0, xmax))\n",
        "    fig.colorbar(im, ax=axs)\n",
        "    plt.show(block=False)"
      ],
      "metadata": {
        "id": "re5VYgn7UP0b"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read dataset in and downsample / transform / pad if needed\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import os\n",
        "\n",
        "class VoiceBankDemand(Dataset):\n",
        "    def __init__(self, clean_dir, noisy_dir, list_dir, len_sec = None, downsample = None, transform = None ):\n",
        "        self.clean_dir = clean_dir\n",
        "        self.noisy_dir = noisy_dir\n",
        "        self.list_dir = list_dir\n",
        "        self.len_sec = len_sec\n",
        "        self.downsample = downsample\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.list_dir)\n",
        "\n",
        "    def __getitem__( self, idx ):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        clean_name = os.path.join( self.clean_dir, self.list_dir[idx] )\n",
        "        noisy_name = os.path.join( self.noisy_dir, self.list_dir[idx] )\n",
        "        clean_audio, fs = ta.load(clean_name)\n",
        "        noisy_audio, fs= ta.load(noisy_name)\n",
        "\n",
        "        if self.downsample:\n",
        "            downsampler = ta.transforms.Resample(fs,self.downsample)\n",
        "            clean_audio = downsampler( clean_audio )\n",
        "            noisy_audio = downsampler( noisy_audio )\n",
        "\n",
        "        if self.len_sec:\n",
        "            num_samples = self.len_sec * self.downsample\n",
        "            if clean_audio.shape[1] > num_samples:\n",
        "                clean_audio = clean_audio[:,0:num_samples]\n",
        "                noisy_audio = noisy_audio[:,0:num_samples]\n",
        "            elif clean_audio.shape[1] < num_samples:\n",
        "                pad_len = int( num_samples - clean_audio.shape[1] )\n",
        "                pad = torch.zeros(1,pad_len)\n",
        "                clean_audio = torch.cat((clean_audio,pad), dim=1)\n",
        "                noisy_audio = torch.cat((noisy_audio,pad),dim=1)\n",
        "\n",
        "        if self.transform:\n",
        "            # apply time-frequency transform to audio (STFT, DCT, DWT)\n",
        "            clean_audio = self.transform( clean_audio ).squeeze()\n",
        "            noisy_audio = self.transform( noisy_audio ).squeeze()\n",
        "\n",
        "        sample = {clean_audio,noisy_audio}\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_clean_path = './DL_Speech_Enhancement/clean_trainset_28spk_wav'\n",
        "train_noisy_path = './DL_Speech_Enhancement/noisy_trainset_28spk_wav'\n",
        "\n",
        "list_dir = os.listdir(train_clean_path)\n",
        "target_fs = 16000 # downsample to 16 KHz\n",
        "spectrogram = ta.transforms.Spectrogram(\n",
        "    n_fft=512,\n",
        "    power=1.0 )\n",
        "\n",
        "training_set = VoiceBankDemand( clean_dir = train_clean_path,\n",
        "                                noisy_dir = train_noisy_path,\n",
        "                                list_dir = list_dir,\n",
        "                                len_sec = 5.0, # clip or pad samples to be 5s\n",
        "                                downsample = 16000, # downsample to 16Khz\n",
        "                                transform = spectrogram )\n",
        "\n",
        "\n",
        "clean, noisy = training_set[0]\n",
        "clean1, noisy1 = training_set[1]\n",
        "\n",
        "#plot_spectrogram(clean)\n",
        "#plot_spectrogram(noisy)\n",
        "\n"
      ],
      "metadata": {
        "id": "iYwWekZk-4kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# U-Net Style Autoencoder from to start from\n",
        "# https://medium.com/@sriskandaryan/autoencoders-demystified-audio-signal-denoising-32a491ab023a\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, chnls_in=1, chnls_out=1):\n",
        "        super(UNetGenerator, self).__init__()\n",
        "        self.down_conv_layer_1 = DownConvBlock(chnls_in, 64, norm=False)\n",
        "        self.down_conv_layer_2 = DownConvBlock(64, 128)\n",
        "        self.down_conv_layer_3 = DownConvBlock(128, 256)\n",
        "        self.down_conv_layer_4 = DownConvBlock(256, 256, dropout=0.5)\n",
        "        self.down_conv_layer_5 = DownConvBlock(256, 256, dropout=0.5)\n",
        "        self.down_conv_layer_6 = DownConvBlock(256, 256, dropout=0.5)\n",
        "\n",
        "        self.up_conv_layer_1 = UpConvBlock(256, 256, kernel_size=(2,3), stride=2, padding=0, dropout=0.5)# 256+256 6 5 kernel_size=(2, 3), stride=2, padding=0\n",
        "        self.up_conv_layer_2 = UpConvBlock(512, 256, kernel_size=(2,3), stride=2, padding=0, dropout=0.5) # 256+256 1 4\n",
        "        self.up_conv_layer_3 = UpConvBlock(512, 256, kernel_size=(2,3), stride=2, padding=0, dropout=0.5) # 2 3\n",
        "        self.up_conv_layer_4 = UpConvBlock(512, 128, dropout=0.5) # 3 2\n",
        "        self.up_conv_layer_5 = UpConvBlock(256, 64) # 4 1\n",
        "        self.up_conv_layer_6 = UpConvBlock(512, 128)\n",
        "        self.up_conv_layer_7 = UpConvBlock(256, 64)\n",
        "        self.upsample_layer = nn.Upsample(scale_factor=2)\n",
        "        self.zero_pad = nn.ZeroPad2d((1, 0, 1, 0))\n",
        "        self.conv_layer_1 = nn.Conv2d(128, chnls_out, 4, padding=1)\n",
        "        self.activation = nn.Tanh()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        enc1 = self.down_conv_layer_1(x)\n",
        "        enc2 = self.down_conv_layer_2(enc1) \n",
        "        enc3 = self.down_conv_layer_3(enc2)\n",
        "        enc4 = self.down_conv_layer_4(enc3)\n",
        "        enc5 = self.down_conv_layer_5(enc4)\n",
        "        enc6 = self.down_conv_layer_6(enc5)\n",
        " \n",
        "        dec1 = self.up_conv_layer_1(enc6, enc5)\n",
        "        dec2 = self.up_conv_layer_2(dec1, enc4)\n",
        "        dec3 = self.up_conv_layer_3(dec2, enc3)\n",
        "        dec4 = self.up_conv_layer_4(dec3, enc2)\n",
        "        dec5 = self.up_conv_layer_5(dec4, enc1)\n",
        "\n",
        "        final = self.upsample_layer(dec5)\n",
        "        final = self.zero_pad(final)\n",
        "        final = self.conv_layer_1(final)\n",
        "        return final\n",
        "\n",
        "class UpConvBlock(nn.Module):\n",
        "    def __init__(self, ip_sz, op_sz, kernel_size=4, stride= 2, padding=1 ,dropout=0.0):\n",
        "        super(UpConvBlock, self).__init__()\n",
        "        self.layers = [\n",
        "            nn.ConvTranspose2d(ip_sz, op_sz, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            nn.InstanceNorm2d(op_sz),\n",
        "            nn.ReLU(),\n",
        "        ]\n",
        "        if dropout:\n",
        "            self.layers += [nn.Dropout(dropout)]\n",
        "    def forward(self, x, enc_ip):\n",
        "        x = nn.Sequential(*(self.layers))(x)\n",
        "        op = torch.cat((x, enc_ip), 1)\n",
        "        return op\n",
        "\n",
        "\n",
        "class DownConvBlock(nn.Module):\n",
        "    def __init__(self, ip_sz, op_sz, kernel_size=4, norm=True, dropout=0.0):\n",
        "        super(DownConvBlock, self).__init__()\n",
        "        self.layers = [nn.Conv2d(ip_sz, op_sz, kernel_size, 2, 1)]\n",
        "        if norm:\n",
        "            self.layers.append(nn.InstanceNorm2d(op_sz))\n",
        "        self.layers += [nn.LeakyReLU(0.2)]\n",
        "        if dropout:\n",
        "            self.layers += [nn.Dropout(dropout)]\n",
        "    def forward(self, x):\n",
        "        op = nn.Sequential(*(self.layers))(x)\n",
        "        return op\n",
        "    \n",
        "model = UNet()"
      ],
      "metadata": {
        "id": "3dwSrp3PVAcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training (work in progress)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle = True )\n",
        "\n",
        "learning_rate = 0.01\n",
        "num_epochs = 10\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(params_to_update,learning_rate)\n",
        "\n",
        "def train_model(model, dataloader, criterion, optimizer, num_epochs ):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for clean_audio,noisey_audio in dataloader:\n",
        "\n",
        "            # send inputs to gpu\n",
        "            clean_audio = clean_audio.to(device)\n",
        "            noisey_audio = noisey_audio.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            with torch.enable_grad():\n",
        "                # Send the noisey speech sample through network\n",
        "                output = model(noisey_audio)\n",
        "\n",
        "                # compute loss between network output and clean audio\n",
        "                loss = criterion(output, clean_audio)\n",
        "\n",
        "                # backward + optimize \n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs1.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader.dataset)\n",
        "\n",
        "    return model, epoch_loss\n"
      ],
      "metadata": {
        "id": "wq5oFQsLVP4R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}