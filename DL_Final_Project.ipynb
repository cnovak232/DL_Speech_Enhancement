{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOaX8U3ELRVGHTuZ2xol82y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cnovak232/DL_Speech_Enhancement/blob/main/DL_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A9xENf0M-0vZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b60b6bb-a9c1-48b6-b2da-316f3ff8e003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DL_Speech_Enhancement'...\n",
            "remote: Enumerating objects: 6627, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)\u001b[K\rremote: Counting objects:  28% (2/7)\u001b[K\rremote: Counting objects:  42% (3/7)\u001b[K\rremote: Counting objects:  57% (4/7)\u001b[K\rremote: Counting objects:  71% (5/7)\u001b[K\rremote: Counting objects:  85% (6/7)\u001b[K\rremote: Counting objects: 100% (7/7)\u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 6627 (delta 1), reused 4 (delta 1), pack-reused 6620\u001b[K\n",
            "Receiving objects: 100% (6627/6627), 1.40 GiB | 29.85 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "Updating files: 100% (6607/6607), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/cnovak232/DL_Speech_Enhancement.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio as ta\n",
        "from IPython.display import Audio, display\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def play_audio(waveform, sample_rate):\n",
        "    waveform = waveform.numpy()\n",
        "\n",
        "    num_channels, num_frames = waveform.shape\n",
        "    if num_channels == 1:\n",
        "        display(Audio(waveform[0], rate=sample_rate))\n",
        "    elif num_channels == 2:\n",
        "        display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n",
        "    else:\n",
        "        raise ValueError(\"Waveform with more than 2 channels are not supported.\")\n",
        "\n",
        "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
        "    waveform = waveform.numpy()\n",
        "\n",
        "    num_channels, num_frames = waveform.shape\n",
        "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
        "\n",
        "    figure, axes = plt.subplots(num_channels, 1)\n",
        "    if num_channels == 1:\n",
        "        axes = [axes]\n",
        "    for c in range(num_channels):\n",
        "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
        "        axes[c].grid(True)\n",
        "    if num_channels > 1:\n",
        "        axes[c].set_ylabel(f'Channel {c+1}')\n",
        "    if xlim:\n",
        "        axes[c].set_xlim(xlim)\n",
        "    if ylim:\n",
        "        axes[c].set_ylim(ylim)\n",
        "\n",
        "    figure.suptitle(title)\n",
        "    plt.show(block=False)\n",
        "\n",
        "def get_spectrogram(\n",
        "    waveform = None,\n",
        "    n_fft = 512,\n",
        "    win_len = None,\n",
        "    hop_len = None,\n",
        "    power = 1.0 ):\n",
        "    spectrogram = ta.transforms.Spectrogram(\n",
        "      n_fft=n_fft,\n",
        "      win_length=win_len,\n",
        "      hop_length=hop_len,\n",
        "      center=True,\n",
        "      pad_mode=\"reflect\",\n",
        "      power=power )\n",
        "    \n",
        "    return spectrogram(waveform)\n",
        "\n",
        "def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
        "    fig, axs = plt.subplots(1, 1)\n",
        "    axs.set_title(title or 'Spectrogram (db)')\n",
        "    axs.set_ylabel(ylabel)\n",
        "    axs.set_xlabel('frame')\n",
        "    im = axs.imshow(ta.function.amplitude_to_DB(spec), origin='lower', aspect=aspect)\n",
        "    if xmax:\n",
        "        axs.set_xlim((0, xmax))\n",
        "    fig.colorbar(im, ax=axs)\n",
        "    plt.show(block=False)"
      ],
      "metadata": {
        "id": "re5VYgn7UP0b"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "\n",
        "train_clean_path = './DL_Speech_Enhancement/clean_trainset_28spk_wav'\n",
        "train_noisy_path = './DL_Speech_Enhancement/noisy_trainset_28spk_wav'\n",
        "\n",
        "list_dir = os.listdir(train_clean_path)\n",
        "old_fs = 48000\n",
        "fs = 16000\n",
        "downsampler = ta.transforms.Resample(old_fs,fs)\n",
        "\n",
        "class VoiceBankDemand(Dataset):\n",
        "    def __init__(self, clean_dir,noisy_dir,list_dir,downsample=None):\n",
        "        self.clean_dir = clean_dir\n",
        "        self.noisy_dir = noisy_dir\n",
        "        self.list_dir = list_dir\n",
        "        self.downsample = downsample\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.list_dir)\n",
        "\n",
        "    def __getitem__( self, idx ):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        clean_name = os.path.join( self.clean_dir, self.list_dir[idx] )\n",
        "        noisy_name = os.path.join( self.noisy_dir, self.list_dir[idx] )\n",
        "        clean_audio, fs = ta.load(clean_name)\n",
        "        noisy_audio, fs= ta.load(noisy_name)\n",
        "\n",
        "        if self.downsample:\n",
        "            clean_audio = self.downsample(clean_audio)\n",
        "            noisy_audio = self.downsample(noisy_audio)\n",
        "\n",
        "        sample = {clean_audio,noisy_audio}\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "training_set = VoiceBankDemand( clean_dir = train_clean_path,\n",
        "                                noisy_dir = train_noisy_path,\n",
        "                                list_dir = list_dir,\n",
        "                                downsample = downsampler )\n",
        "\n",
        "clean, noisy = training_set[0]\n",
        "\n",
        "play_audio(clean,fs)\n",
        "play_audio(noisy,fs)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iYwWekZk-4kT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}