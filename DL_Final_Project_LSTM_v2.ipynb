{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cnovak232/DL_Speech_Enhancement/blob/LSTM/DL_Final_Project_LSTM_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data by cloning the repo - easiest way to access shared data"
      ],
      "metadata": {
        "id": "kA2ws-l-yuS9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9xENf0M-0vZ"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/cnovak232/DL_Speech_Enhancement.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define some helper function for plotting and playing audio"
      ],
      "metadata": {
        "id": "Q1RLKW7oy2Yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio as ta\n",
        "import librosa as lib\n",
        "from IPython.display import Audio, display\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# helper functions for audio and what not\n",
        "# mostly taken for torchaudio tutorials \n",
        "\n",
        "def play_audio(waveform, sample_rate):\n",
        "    waveform = waveform.numpy()\n",
        "\n",
        "    num_channels, num_frames = waveform.shape\n",
        "    if num_channels == 1:\n",
        "        display(Audio(waveform[0], rate=sample_rate))\n",
        "    elif num_channels == 2:\n",
        "        display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n",
        "    else:\n",
        "        raise ValueError(\"Waveform with more than 2 channels are not supported.\")\n",
        "\n",
        "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
        "    waveform = waveform.numpy()\n",
        "\n",
        "    num_channels, num_frames = waveform.shape\n",
        "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
        "\n",
        "    figure, axes = plt.subplots(num_channels, 1)\n",
        "    if num_channels == 1:\n",
        "        axes = [axes]\n",
        "    for c in range(num_channels):\n",
        "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
        "        axes[c].grid(True)\n",
        "    if num_channels > 1:\n",
        "        axes[c].set_ylabel(f'Channel {c+1}')\n",
        "    if xlim:\n",
        "        axes[c].set_xlim(xlim)\n",
        "    if ylim:\n",
        "        axes[c].set_ylim(ylim)\n",
        "\n",
        "    figure.suptitle(title)\n",
        "    plt.show(block=False)\n",
        "\n",
        "def get_spectrogram(\n",
        "    waveform = None,\n",
        "    n_fft = 512,\n",
        "    win_len = None,\n",
        "    hop_len = None,\n",
        "    power = 1.0 ):\n",
        "    spectrogram = ta.transforms.Spectrogram(\n",
        "      n_fft=n_fft,\n",
        "      win_length=win_len,\n",
        "      hop_length=hop_len,\n",
        "      center=True,\n",
        "      pad_mode=\"reflect\",\n",
        "      power=power )\n",
        "    \n",
        "    return spectrogram(waveform)\n",
        "\n",
        "def plot_spectrogram(spec, type = \"amplitude\", title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
        "    fig, axs = plt.subplots(1, 1)\n",
        "    axs.set_title(title or 'Spectrogram (db)')\n",
        "    axs.set_ylabel(ylabel)\n",
        "    axs.set_xlabel('frame')\n",
        "    toDb = ta.transforms.AmplitudeToDB(type)\n",
        "    im = axs.imshow(spec, origin='lower', aspect=aspect)\n",
        "    if xmax:\n",
        "        axs.set_xlim((0, xmax))\n",
        "    fig.colorbar(im, ax=axs)\n",
        "    plt.show(block=False)\n",
        "\n",
        "def norm_spec( spec ):\n",
        "    normed = spec / spec.max()\n",
        "    return normed, spec.max()\n"
      ],
      "metadata": {
        "id": "re5VYgn7UP0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a Custom Dataset class for the Data and read it in"
      ],
      "metadata": {
        "id": "ryD_nLFqy7zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read dataset in and downsample / transform / pad if needed\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio as ta\n",
        "\n",
        "class VoiceBankDemand(Dataset):\n",
        "    def __init__(self, clean_dir, noisy_dir, list_dir, \n",
        "                 data = \"train\", len_samples = None, downsample = None, \n",
        "                 transform = None ):\n",
        "        self.clean_dir = clean_dir\n",
        "        self.noisy_dir = noisy_dir\n",
        "        self.list_dir = list_dir\n",
        "        self.num_samples = len_samples\n",
        "        self.downsample = downsample\n",
        "        self.transform = transform\n",
        "        self.data = data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.list_dir)\n",
        "\n",
        "    def __getitem__( self, idx ):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        clean_name = os.path.join( self.clean_dir, self.list_dir[idx] )\n",
        "        noisy_name = os.path.join( self.noisy_dir, self.list_dir[idx] )\n",
        "        clean_audio, fs = ta.load(clean_name)\n",
        "        noisy_audio, fs= ta.load(noisy_name)\n",
        "\n",
        "        if self.downsample:\n",
        "            downsampler = ta.transforms.Resample(fs,self.downsample)\n",
        "            clean_audio = downsampler( clean_audio )\n",
        "            noisy_audio = downsampler( noisy_audio )\n",
        "\n",
        "        if self.num_samples:\n",
        "            orig_len = clean_audio.shape[1]\n",
        "            if clean_audio.shape[1] > num_samples:\n",
        "                clean_audio = clean_audio[:,:num_samples]\n",
        "                noisy_audio = noisy_audio[:,:num_samples]\n",
        "            elif clean_audio.shape[1] < num_samples:\n",
        "                pad_len = int( num_samples - clean_audio.shape[1] )\n",
        "                pad = torch.zeros(1,pad_len)\n",
        "                clean_audio = torch.cat((clean_audio,pad), dim=1)\n",
        "                noisy_audio = torch.cat((noisy_audio,pad),dim=1)\n",
        "        \n",
        "        if self.data == \"test\":\n",
        "            if self.transform:\n",
        "                noisy_trnsfrm = self.transform( noisy_audio )\n",
        "                clean_trnsfrm = self.transform( clean_audio )\n",
        "            clean_mag,_ = norm_spec( torch.abs(clean_trnsfrm) )\n",
        "            clean_audio = (clean_audio, clean_mag)\n",
        "            noisy_mag, norm_val = norm_spec( torch.abs(noisy_trnsfrm) )\n",
        "            noisy_phase = torch.angle(noisy_trnsfrm)\n",
        "            noisy_audio = (noisy_mag, noisy_phase, noisy_audio, norm_val)\n",
        "        else:\n",
        "            if self.transform:\n",
        "                clean_audio,_ = norm_spec( self.transform( clean_audio ) )\n",
        "                noisy_audio,_ = norm_spec( self.transform( noisy_audio ) )\n",
        "\n",
        "        sample = (clean_audio, noisy_audio, orig_len)\n",
        "\n",
        "        return sample\n",
        "\n",
        "train_clean_path = './DL_Speech_Enhancement/clean_trainset_28spk_wav'\n",
        "train_noisy_path = './DL_Speech_Enhancement/noisy_trainset_28spk_wav'\n",
        "test_clean_path  = './DL_Speech_Enhancement/clean_testset_wav'\n",
        "test_noisy_path  = './DL_Speech_Enhancement/noisy_testset_wav'\n",
        "\n",
        "list_dir_train = os.listdir(train_clean_path)\n",
        "list_dir_test = os.listdir(test_clean_path)\n",
        "\n",
        "target_fs = 16000 # downsample to 16 KHz\n",
        "spectrogram = ta.transforms.Spectrogram(\n",
        "    n_fft=512,\n",
        "    power=1.0,\n",
        "    normalized = False )\n",
        "complex_spec = ta.transforms.Spectrogram(\n",
        "    n_fft=512,\n",
        "    power=None,\n",
        "    normalized = False ) # return complex spectrum\n",
        "\n",
        "num_samples = int( 5.0 * target_fs ) \n",
        "\n",
        "train_set = VoiceBankDemand( clean_dir = train_clean_path,\n",
        "                             noisy_dir = train_noisy_path,\n",
        "                             list_dir = list_dir_train,\n",
        "                             len_samples = num_samples, # clip or pad samples to be 5s\n",
        "                             downsample = target_fs, # downsample to 16Khz\n",
        "                             transform = spectrogram )\n",
        "\n",
        "# returns the mag/phase of each audio file \n",
        "test_set = VoiceBankDemand( clean_dir = test_clean_path,\n",
        "                            noisy_dir = test_noisy_path,\n",
        "                            list_dir = list_dir_test,\n",
        "                            data = \"test\",\n",
        "                            len_samples = num_samples,\n",
        "                            downsample = target_fs,\n",
        "                            transform = complex_spec )\n",
        "\n",
        "\n",
        "clean, noisy, orig_len = train_set[1]\n",
        "print(clean.size())\n",
        "print(orig_len)\n",
        "\n",
        "clean_test, noisy_test, orig_len = test_set[0]\n",
        "\n",
        "print(len(noisy_test))\n",
        "\n",
        "noisy_mag, noisy_phase, noisy_wav, norm_val = noisy_test\n",
        "\n",
        "plot_spectrogram(noisy.squeeze())\n",
        "plot_spectrogram(noisy_mag.squeeze())"
      ],
      "metadata": {
        "id": "5t-r8rZMbSZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio as ta\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "####################################\n",
        "\n",
        "# Using Reccurnt Neural Network (RNN) based method - In recurrent networks, each component shares the same weights. (Feedforward network - each input has its own weights)\n",
        "# RNNs maintain a hidden state that can capture information from previous time steps, allowing them to model temporal dependencies in the input data.\n",
        "# Also, a model can process sequences with different lengths by sharing the weights. \n",
        "# Note: if you want variable lengths, you could also use a Gated Recurrent Unit (GRU) (more computationally efdficient than LSTM but may perform worse)\n",
        "\n",
        "# LSTM block with LSTM layer and fully connected layer\n",
        "# Define the SpeechEnhancementLSTM class\n",
        "class SpeechEnhancementLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(SpeechEnhancementLSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, input_size * 257)  # Update the output size of the linear layer\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # Initialize hidden state and cell state with zeros\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Pack the input sequence\n",
        "        x_packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        out_packed, _ = self.lstm(x_packed, (h0, c0))\n",
        "\n",
        "        # Unpack the sequence\n",
        "        out, _ = nn.utils.rnn.pad_packed_sequence(out_packed, batch_first=True)\n",
        "\n",
        "        # Apply the fully connected layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # Reshape the output to match the input dimensions\n",
        "        out = out.view(x.size(0), x.size(1), 257)\n",
        "        \n",
        "        return out\n",
        "\n",
        "###############################################\n",
        "\n"
      ],
      "metadata": {
        "id": "iYwWekZk-4kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main block for training the model and testing.\n",
        "Initializes all training and testing parameters. \n",
        "Performs training and testing in one big loop, testing each epoch to track any overfitting."
      ],
      "metadata": {
        "id": "3DhbW9p3yyaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch torchvision\n",
        "\n",
        "# training (work in progress)\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import os\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
        "torch.cuda.synchronize()\n",
        "#torch.backends.cudnn.enabled = False\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "# Set device to use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train_model(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Iterate over data\n",
        "    for clean_audio, noisy_audio, orig_len in dataloader:\n",
        "        # Send inputs to the device (GPU or CPU)\n",
        "        clean_audio = clean_audio.to(device)\n",
        "        noisy_audio = noisy_audio.to(device)\n",
        "\n",
        "        # Remove extra dimension and transpose the input and target tensors\n",
        "        clean_audio = clean_audio.squeeze(1).permute(0, 2, 1)\n",
        "        noisy_audio = noisy_audio.squeeze(1).permute(0, 2, 1)\n",
        "\n",
        "        lengths = orig_len.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.enable_grad():\n",
        "            # Send the noisy speech sample through the network\n",
        "            output = model(noisy_audio)\n",
        "\n",
        "            # Transpose the output tensor back to its original shape\n",
        "            output = output.permute(0, 2, 1)\n",
        "\n",
        "            # Compute loss between network output and clean audio\n",
        "            loss = criterion(output, clean_audio)\n",
        "\n",
        "            # Backward + optimize \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "\n",
        "    return model, epoch_loss\n",
        "\n",
        "\n",
        "def test_model(model, device, criterion, dataloader):\n",
        "    model.eval() \n",
        "\n",
        "    cleaned_waveforms = torch.empty(0)\n",
        "    cleaned_data = torch.empty(0)\n",
        "    running_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for clean_audio, noisy_audio, orig_len in dataloader:\n",
        "            # send inputs to the device (GPU or CPU)\n",
        "            clean_audio = clean_audio.to(device)\n",
        "            noisy_audio = noisy_audio.to(device)\n",
        "\n",
        "            # Remove extra dimension and transpose the input and target tensors\n",
        "            clean_audio = clean_audio.permute(0, 2, 1)\n",
        "            noisy_audio = noisy_audio.squeeze(2)\n",
        "\n",
        "            lengths = orig_len.to(device)\n",
        "\n",
        "            # Pack sequences\n",
        "            packed_noisy = nn.utils.rnn.pack_padded_sequence(noisy_audio, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "\n",
        "            # Forward\n",
        "            packed_enhanced = model(packed_noisy, lengths)\n",
        "\n",
        "            # Unpack sequences\n",
        "            enhanced_data, lengths = nn.utils.rnn.pad_packed_sequence(packed_enhanced, batch_first=True, total_length=noisy_audio.shape[1], lengths=lengths)\n",
        "\n",
        "            loss = criterion(enhanced_data, clean_audio)\n",
        "\n",
        "            cleaned_data = torch.cat((cleaned_data, enhanced_data.detach().cpu()), 0)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    test_loss = running_loss / len(dataloader.dataset)\n",
        "\n",
        "    cleaned_data = torch.stack([x.squeeze(0).permute(1, 0) for x in cleaned_data])\n",
        "    \n",
        "    return cleaned_data, test_loss\n",
        "\n",
        "\n",
        "      \n",
        "train_loader = DataLoader(train_set, batch_size=128)\n",
        "test_loader = DataLoader(test_set, batch_size=128)\n",
        "\n",
        "# Get the input shape from the dataset\n",
        "input_shape = (313, 257)\n",
        "output_shape = (313, 257)\n",
        "hidden_shape = (128, 2)\n",
        "num_layers = 2\n",
        "\n",
        "# Assuming you have created a DataLoader named `train_loader`\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assuming you have created a DataLoader named `train_loader`\n",
        "input_size = 257  # Assuming 80 features for each time step\n",
        "hidden_size = 4\n",
        "num_layers = 2\n",
        "output_size = 80  # Assuming 80 features for the output\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = SpeechEnhancementLSTM(input_size, hidden_size, num_layers).to(device)\n",
        "\n",
        "# Define your loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Call this function to free up GPU memory cache\n",
        "#torch.cuda.empty_cache()\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# Training Loop\n",
        "accumulation_steps = 4  # Accumulate gradients over 4 batches\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for batch_idx, (clean_audio, noisy_audio, lengths) in enumerate(train_loader):\n",
        "        # Move data and lengths to the device\n",
        "        clean_audio = clean_audio.to(device)\n",
        "        noisy_audio = noisy_audio.to(device)\n",
        "        lengths = lengths.to(device)\n",
        "\n",
        "        lengths = lengths.cpu()\n",
        "        noisy_audio = noisy_audio.squeeze(1).permute(0, 2, 1)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(noisy_audio, lengths)\n",
        "\n",
        "        # Slice to the original sequence lengths\n",
        "       # output = output[:, :length.item()]\n",
        "        print(\"Output tensor size:\", output.size())\n",
        "\n",
        "\n",
        "        # Reshape the output tensor to match the target tensor shape\n",
        "        output = output.view(clean_audio.shape)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(output.data, clean_audio.data)\n",
        "\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Accumulate the gradients\n",
        "        if (batch_idx + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # Update the running loss\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print the average loss for this epoch\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "    # Perform optimizer step for any remaining accumulated gradients\n",
        "    if batch_idx % accumulation_steps != 0:\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "print(\"Finished training.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "wq5oFQsLVP4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics[audio]\n",
        "!pip install pesq"
      ],
      "metadata": {
        "id": "6IBSOg47iL9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loop through testing samples and evaluate enhanced data against clean data"
      ],
      "metadata": {
        "id": "XCimwEXBZzOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import SignalNoiseRatio\n",
        "from pesq import pesq\n",
        "\n",
        "SNR = SignalNoiseRatio(zero_mean=True)\n",
        "\n",
        "snr_vals_noisy = torch.empty(len(test_set))\n",
        "pesq_vals_noisy = torch.empty(len(test_set))\n",
        "snr_vals = torch.empty(len(test_set))\n",
        "pesq_vals = torch.empty(len(test_set))\n",
        "\n",
        "for i in range( len(test_set) ):\n",
        "    clean_audio,noisy_audio,orig_len = test_set[i]\n",
        "    clean_wav = clean_audio\n",
        "    #noisy_audio = noisy_data[2]\n",
        "    enhanced_audio = cleaned_data[i]\n",
        "\n",
        "    #un_norm_mag = torch.transpose(enhanced_mag,0,3) * norm_val\n",
        "    #un_norm_mag = torch.transpose(un_norm_mag,0,3)\n",
        "    #complex_out = torch.polar(un_norm_mag, noisy_phase)\n",
        "    enhanced_audio = enhanced_audio.detach().cpu()\n",
        "\n",
        "    #if inv_transform:\n",
        "        #audio_enhanced = inv_transform(audio_enhanced, clean_wav.shape[2])\n",
        "\n",
        "    if enhanced_audio.shape[1] > orig_len:\n",
        "        enhanced_audio = enhanced_audio[:,:orig_len]\n",
        "        noisy_audio = noisy_audio[:,:orig_len]\n",
        "        clean_wav = clean_wav[:,:orig_len]\n",
        "\n",
        "    snr_vals[i] = SNR(enhanced_audio, clean_wav)\n",
        "    pesq_vals[i] = pesq( target_fs, enhanced_audio.squeeze().numpy(), clean_wav.squeeze().numpy(), \"nb\")\n",
        "\n",
        "    snr_vals_noisy[i] = SNR(noisy_audio, clean_wav)\n",
        "    pesq_vals_noisy[i] = pesq( target_fs, noisy_audio.squeeze().numpy(), clean_wav.squeeze().numpy(), \"nb\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xUkP-YBVe1_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute average and best SNR and PESQ improvements"
      ],
      "metadata": {
        "id": "cyg1ZFIkZQKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SNR_imp = snr_vals - snr_vals_noisy\n",
        "PESQ_imp = pesq_vals - pesq_vals_noisy\n",
        "print(\"Average PESQ Enhanced\", pesq_vals.sum() / len(pesq_vals) )\n",
        "print(\"Average PESQ Noisy\", pesq_vals_noisy.sum() / len(pesq_vals_noisy) )\n",
        "\n",
        "avg_SNR_imp = SNR_imp.sum() / len(SNR_imp)\n",
        "avg_PESQ_imp = PESQ_imp.sum() / len(PESQ_imp)\n",
        "\n",
        "peak_SNR_imp = SNR_imp.max()\n",
        "peak_snr_ind = SNR_imp.argmax()\n",
        "peak_PESQ_imp = PESQ_imp.max()\n",
        "peak_PESQ_ind = PESQ_imp.argmax()\n",
        "\n",
        "print(\"Average SNR Improvement = \", avg_SNR_imp )\n",
        "print(\"Average PESQ Improvement = \", avg_PESQ_imp )\n",
        "\n",
        "print(\"Best SNR Improvement = \", peak_SNR_imp )\n",
        "print(\"Best PESQ Improvment = \", peak_PESQ_imp )\n",
        "\n",
        "\n",
        "# use if using magnitude spectrograms\n",
        "#plot_spectrogram(cleaned_data[0].squeeze())\n",
        "#plot_spectrogram(test_set[0][0][1].squeeze())\n",
        "\n",
        "play_audio(test_set[0][1],target_fs) # play noisy test sample\n",
        "play_audio(cleaned_data[0],target_fs)\n"
      ],
      "metadata": {
        "id": "cIfbEp1hjocU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}