{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO3hBUG5d0O61jqhaln2I0L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cnovak232/DL_Speech_Enhancement/blob/main/DL_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data by cloning the repo - easiest way to access shared data"
      ],
      "metadata": {
        "id": "kA2ws-l-yuS9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9xENf0M-0vZ"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/cnovak232/DL_Speech_Enhancement.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define some helper function for plotting and playing audio"
      ],
      "metadata": {
        "id": "Q1RLKW7oy2Yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio as ta\n",
        "import librosa as lib\n",
        "from IPython.display import Audio, display\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# helper functions for audio and what not\n",
        "# mostly taken for torchaudio tutorials \n",
        "\n",
        "def play_audio(waveform, sample_rate):\n",
        "    waveform = waveform.numpy()\n",
        "\n",
        "    num_channels, num_frames = waveform.shape\n",
        "    if num_channels == 1:\n",
        "        display(Audio(waveform[0], rate=sample_rate))\n",
        "    elif num_channels == 2:\n",
        "        display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n",
        "    else:\n",
        "        raise ValueError(\"Waveform with more than 2 channels are not supported.\")\n",
        "\n",
        "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
        "    waveform = waveform.numpy()\n",
        "\n",
        "    num_channels, num_frames = waveform.shape\n",
        "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
        "\n",
        "    figure, axes = plt.subplots(num_channels, 1)\n",
        "    if num_channels == 1:\n",
        "        axes = [axes]\n",
        "    for c in range(num_channels):\n",
        "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
        "        axes[c].grid(True)\n",
        "    if num_channels > 1:\n",
        "        axes[c].set_ylabel(f'Channel {c+1}')\n",
        "    if xlim:\n",
        "        axes[c].set_xlim(xlim)\n",
        "    if ylim:\n",
        "        axes[c].set_ylim(ylim)\n",
        "\n",
        "    figure.suptitle(title)\n",
        "    plt.show(block=False)\n",
        "\n",
        "def get_spectrogram(\n",
        "    waveform = None,\n",
        "    n_fft = 512,\n",
        "    win_len = None,\n",
        "    hop_len = None,\n",
        "    power = 1.0 ):\n",
        "    spectrogram = ta.transforms.Spectrogram(\n",
        "      n_fft=n_fft,\n",
        "      win_length=win_len,\n",
        "      hop_length=hop_len,\n",
        "      center=True,\n",
        "      pad_mode=\"reflect\",\n",
        "      power=power )\n",
        "    \n",
        "    return spectrogram(waveform)\n",
        "\n",
        "def plot_spectrogram(spec, type = \"amplitude\", title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
        "    fig, axs = plt.subplots(1, 1)\n",
        "    axs.set_title(title or 'Spectrogram (db)')\n",
        "    axs.set_ylabel(ylabel)\n",
        "    axs.set_xlabel('frame')\n",
        "    toDb = ta.transforms.AmplitudeToDB(type)\n",
        "    im = axs.imshow(spec, origin='lower', aspect=aspect)\n",
        "    if xmax:\n",
        "        axs.set_xlim((0, xmax))\n",
        "    fig.colorbar(im, ax=axs)\n",
        "    plt.show(block=False)\n",
        "\n",
        "def norm_spec( spec ):\n",
        "    normed = spec / spec.max()\n",
        "    return normed, spec.max()\n"
      ],
      "metadata": {
        "id": "re5VYgn7UP0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a Custom Dataset class for the Data and read it in"
      ],
      "metadata": {
        "id": "ryD_nLFqy7zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read dataset in and downsample / transform / pad if needed\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import os\n",
        "\n",
        "class VoiceBankDemand(Dataset):\n",
        "    def __init__(self, clean_dir, noisy_dir, list_dir, \n",
        "                 data = \"train\", len_samples = None, downsample = None, \n",
        "                 transform = None ):\n",
        "        self.clean_dir = clean_dir\n",
        "        self.noisy_dir = noisy_dir\n",
        "        self.list_dir = list_dir\n",
        "        self.num_samples = len_samples\n",
        "        self.downsample = downsample\n",
        "        self.transform = transform\n",
        "        self.data = data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.list_dir)\n",
        "\n",
        "    def __getitem__( self, idx ):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        clean_name = os.path.join( self.clean_dir, self.list_dir[idx] )\n",
        "        noisy_name = os.path.join( self.noisy_dir, self.list_dir[idx] )\n",
        "        clean_audio, fs = ta.load(clean_name)\n",
        "        noisy_audio, fs= ta.load(noisy_name)\n",
        "\n",
        "        if self.downsample:\n",
        "            downsampler = ta.transforms.Resample(fs,self.downsample)\n",
        "            clean_audio = downsampler( clean_audio )\n",
        "            noisy_audio = downsampler( noisy_audio )\n",
        "\n",
        "        if self.num_samples:\n",
        "            orig_len = clean_audio.shape[1]\n",
        "            if clean_audio.shape[1] > num_samples:\n",
        "                clean_audio = clean_audio[:,:num_samples]\n",
        "                noisy_audio = noisy_audio[:,:num_samples]\n",
        "            elif clean_audio.shape[1] < num_samples:\n",
        "                pad_len = int( num_samples - clean_audio.shape[1] )\n",
        "                pad = torch.zeros(1,pad_len)\n",
        "                clean_audio = torch.cat((clean_audio,pad), dim=1)\n",
        "                noisy_audio = torch.cat((noisy_audio,pad),dim=1)\n",
        "        \n",
        "        if self.data == \"test\":\n",
        "            if self.transform:\n",
        "                noisy_trnsfrm = self.transform( noisy_audio )\n",
        "                clean_trnsfrm = self.transform( clean_audio )\n",
        "            clean_mag,_ = norm_spec( torch.abs(clean_trnsfrm) )\n",
        "            clean_audio = (clean_audio, clean_mag)\n",
        "            noisy_mag, norm_val = norm_spec( torch.abs(noisy_trnsfrm) )\n",
        "            noisy_phase = torch.angle(noisy_trnsfrm)\n",
        "            noisy_audio = (noisy_mag, noisy_phase, noisy_audio, norm_val)\n",
        "        else:\n",
        "            if self.transform:\n",
        "                clean_audio,_ = norm_spec( self.transform( clean_audio ) )\n",
        "                noisy_audio,_ = norm_spec( self.transform( noisy_audio ) )\n",
        "\n",
        "        sample = (clean_audio, noisy_audio, orig_len)\n",
        "\n",
        "        return sample\n",
        "\n",
        "train_clean_path = './DL_Speech_Enhancement/clean_trainset_28spk_wav'\n",
        "train_noisy_path = './DL_Speech_Enhancement/noisy_trainset_28spk_wav'\n",
        "test_clean_path  = './DL_Speech_Enhancement/clean_testset_wav'\n",
        "test_noisy_path  = './DL_Speech_Enhancement/noisy_testset_wav'\n",
        "\n",
        "list_dir_train = os.listdir(train_clean_path)\n",
        "list_dir_test = os.listdir(test_clean_path)\n",
        "\n",
        "target_fs = 16000 # downsample to 16 KHz\n",
        "spectrogram = ta.transforms.Spectrogram(\n",
        "    n_fft=512,\n",
        "    power=1.0,\n",
        "    normalized = False )\n",
        "complex_spec = ta.transforms.Spectrogram(\n",
        "    n_fft=512,\n",
        "    power=None,\n",
        "    normalized = False ) # return complex spectrum\n",
        "\n",
        "num_samples = int( 5.0 * target_fs ) \n",
        "\n",
        "train_set = VoiceBankDemand( clean_dir = train_clean_path,\n",
        "                             noisy_dir = train_noisy_path,\n",
        "                             list_dir = list_dir_train,\n",
        "                             len_samples = num_samples, # clip or pad samples to be 5s\n",
        "                             downsample = target_fs, # downsample to 16Khz\n",
        "                             transform = spectrogram )\n",
        "\n",
        "# returns the mag/phase of each audio file \n",
        "test_set = VoiceBankDemand( clean_dir = test_clean_path,\n",
        "                            noisy_dir = test_noisy_path,\n",
        "                            list_dir = list_dir_test,\n",
        "                            data = \"test\",\n",
        "                            len_samples = num_samples,\n",
        "                            downsample = target_fs,\n",
        "                            transform = complex_spec )\n",
        "\n",
        "\n",
        "clean, noisy, orig_len = train_set[1]\n",
        "print(clean.size())\n",
        "print(orig_len)\n",
        "\n",
        "clean_test, noisy_test, orig_len = test_set[0]\n",
        "\n",
        "noisy_mag, noisy_phase, noisy_audio = noisy_test\n",
        "\n",
        "plot_spectrogram(noisy.squeeze())\n",
        "plot_spectrogram(noisy_mag.squeeze())"
      ],
      "metadata": {
        "id": "iYwWekZk-4kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# U-Net Style Autoencoder from to start from\n",
        "# https://medium.com/@sriskandaryan/autoencoders-demystified-audio-signal-denoising-32a491ab023a\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, chnls_in=1, chnls_out=1):\n",
        "        super(UNet, self).__init__()\n",
        "        self.down_conv_layer_1 = DownConvBlock(chnls_in, 64, norm=False)\n",
        "        self.down_conv_layer_2 = DownConvBlock(64, 128)\n",
        "        self.down_conv_layer_3 = DownConvBlock(128, 256)\n",
        "        self.down_conv_layer_4 = DownConvBlock(256, 256, dropout=0.5)\n",
        "        self.down_conv_layer_5 = DownConvBlock(256, 256, dropout=0.5)\n",
        "        self.down_conv_layer_6 = DownConvBlock(256, 256, dropout=0.5)\n",
        "\n",
        "        self.up_conv_layer_1 = UpConvBlock(256, 256, kernel_size=(2,3), stride=2, padding=0, dropout=0.5)# 256+256 6 5 kernel_size=(2, 3), stride=2, padding=0\n",
        "        self.up_conv_layer_2 = UpConvBlock(512, 256, kernel_size=(2,3), stride=2, padding=0, dropout=0.5) # 256+256 1 4\n",
        "        self.up_conv_layer_3 = UpConvBlock(512, 256, kernel_size=(2,3), stride=2, padding=0, dropout=0.5) # 2 3\n",
        "        self.up_conv_layer_4 = UpConvBlock(512, 128, dropout=0.5) # 3 2\n",
        "        self.up_conv_layer_5 = UpConvBlock(256, 64) # 4 1\n",
        "        self.up_conv_layer_6 = UpConvBlock(512, 128)\n",
        "        self.up_conv_layer_7 = UpConvBlock(256, 64)\n",
        "        self.upsample_layer = nn.Upsample(scale_factor=2)\n",
        "        self.zero_pad = nn.ZeroPad2d((1, 0, 1, 0))\n",
        "        self.conv_layer_1 = nn.Conv2d(128, chnls_out, 4, padding='same')\n",
        "        self.activation = nn.Tanh()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        enc1 = self.down_conv_layer_1(x)\n",
        "        enc2 = self.down_conv_layer_2(enc1) \n",
        "        enc3 = self.down_conv_layer_3(enc2)\n",
        "        enc4 = self.down_conv_layer_4(enc3)\n",
        "        enc5 = self.down_conv_layer_5(enc4)\n",
        "        enc6 = self.down_conv_layer_6(enc5)\n",
        " \n",
        "        dec1 = self.up_conv_layer_1(enc6, enc5)\n",
        "        dec2 = self.up_conv_layer_2(dec1, enc4)\n",
        "        dec3 = self.up_conv_layer_3(dec2, enc3)\n",
        "        dec4 = self.up_conv_layer_4(dec3, enc2)\n",
        "        dec5 = self.up_conv_layer_5(dec4, enc1)\n",
        "\n",
        "        final = self.upsample_layer(dec5)\n",
        "        final = self.zero_pad(final)\n",
        "        final = self.conv_layer_1(final)\n",
        "        return final\n",
        "\n",
        "class UpConvBlock(nn.Module):\n",
        "    def __init__(self, ip_sz, op_sz, kernel_size=4, stride= 2, padding=1 ,dropout=0.0):\n",
        "        super(UpConvBlock, self).__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.ConvTranspose2d(ip_sz, op_sz, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            nn.InstanceNorm2d(op_sz),\n",
        "            nn.ReLU(),\n",
        "        ])\n",
        "        if dropout:\n",
        "            self.layers += [nn.Dropout(dropout)]\n",
        "    def forward(self, x, enc_ip):\n",
        "        x = nn.Sequential(*(self.layers))(x)\n",
        "        op = torch.cat((x, enc_ip), 1)\n",
        "        return op\n",
        "\n",
        "\n",
        "class DownConvBlock(nn.Module):\n",
        "    def __init__(self, ip_sz, op_sz, kernel_size=4, norm=True, dropout=0.0):\n",
        "        super(DownConvBlock, self).__init__()\n",
        "        self.layers = nn.ModuleList([nn.Conv2d(ip_sz, op_sz, kernel_size, 2, 1)])\n",
        "        if norm:\n",
        "            self.layers.append(nn.InstanceNorm2d(op_sz))\n",
        "        self.layers += [nn.LeakyReLU(0.2)]\n",
        "        if dropout:\n",
        "            self.layers += [nn.Dropout(dropout)]\n",
        "    def forward(self, x):\n",
        "        op = nn.Sequential(*(self.layers))(x)\n",
        "        return op"
      ],
      "metadata": {
        "id": "3dwSrp3PVAcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main block for training the model and testing.\n",
        "Initializes all training and testing parameters. \n",
        "Performs training and testing in one big loop, testing each epoch to track any overfitting."
      ],
      "metadata": {
        "id": "3DhbW9p3yyaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training (work in progress)\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_model( model, dataloader, criterion, optimizer, device ):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Iterate over data.\n",
        "    for clean_audio,noisey_audio,orig_len in dataloader:\n",
        "\n",
        "        # send inputs to gpu\n",
        "        clean_audio = clean_audio.to(device)\n",
        "        noisey_audio = noisey_audio.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        with torch.enable_grad():\n",
        "            # Send the noisey speech sample through network\n",
        "            output = model(noisey_audio)\n",
        "\n",
        "            # compute loss between network output and clean audio\n",
        "            loss = criterion(output, clean_audio)\n",
        "\n",
        "            # backward + optimize \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "\n",
        "    return model, epoch_loss\n",
        "\n",
        "def test_model( model, device, criterion, dataloader, fs = 16000, inv_transform = None ):\n",
        "    model.eval() \n",
        "    inv_transfrom = inv_transform.to(device)\n",
        "    cleaned_waveforms = torch.empty(0)\n",
        "    cleaned_mags = torch.empty(0)\n",
        "    running_loss = 0\n",
        "    for clean_audio, noisy_audio, orig_len in dataloader:\n",
        "        noisy_mag, noisy_phase, noisy_wav, norm_val = noisy_audio\n",
        "        noisy_mag = noisy_mag.to(device)\n",
        "        noisy_phase = noisy_phase.to(device)\n",
        "        clean_wav, clean_mag = clean_audio\n",
        "        clean_mag = clean_mag.to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # forward\n",
        "            enhanced_mag = model(noisy_mag)\n",
        "\n",
        "            loss = criterion(enhanced_mag, clean_mag)\n",
        "\n",
        "            complex_out = torch.polar(enhanced_mag * norm_val, noisy_phase)\n",
        "            #audio_enhanced = complex_out.detach().cpu()\n",
        "            \n",
        "            if inv_transform:\n",
        "                audio_enhanced = inv_transform(audio_enhanced, clean_wav.shape[2])\n",
        "\n",
        "            cleaned_waveforms = torch.cat( (cleaned_waveforms, audio_enhanced), 0 )\n",
        "            cleaned_mags = torch.cat( (cleaned_mags, enhanced_mag.detach().cpu()), 0 )\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    test_loss = running_loss / len( dataloader.dataset )\n",
        "\n",
        "    return cleaned_waveforms, cleaned_mags, test_loss\n",
        "\n",
        "\n",
        "train_loader = DataLoader( train_set, batch_size=64 )\n",
        "test_loader = DataLoader( test_set, batch_size=64 )\n",
        "\n",
        "model = UNet()\n",
        "\n",
        "learning_rate = 0.01\n",
        "num_epochs = 10\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(),learning_rate)\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "inv_spec = ta.transforms.InverseSpectrogram(n_fft=512, normalized = False )\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "    print('-' * 10)\n",
        "\n",
        "    model, epoch_loss = train_model( model, train_loader, criterion, optimizer, device )\n",
        "    print('Train Epoch Loss = ', epoch_loss )\n",
        "\n",
        "    cleaned_waveforms, cleaned_mag, test_loss = \\\n",
        "        test_model( model, device, criterion, test_loader, fs = 16000, inv_transform = inv_spec)\n",
        "    print('Test Epoch Loss = ', test_loss )\n"
      ],
      "metadata": {
        "id": "wq5oFQsLVP4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics[audio]\n",
        "!pip install pesq"
      ],
      "metadata": {
        "id": "6IBSOg47iL9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loop through testing samples and evaluate enhanced data against clean data"
      ],
      "metadata": {
        "id": "XCimwEXBZzOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import SignalNoiseRatio\n",
        "from pesq import pesq\n",
        "\n",
        "SNR = SignalNoiseRatio(zero_mean=True)\n",
        "\n",
        "snr_vals_noisy = torch.empty(len(test_set))\n",
        "pesq_vals_noisy = torch.empty(len(test_set))\n",
        "snr_vals = torch.empty(len(test_set))\n",
        "pesq_vals = torch.empty(len(test_set))\n",
        "\n",
        "for i in range( len(test_set) ):\n",
        "    clean_audio,noisy_data, orig_len = test_set[i]\n",
        "    noisy_audio = noisy_data[2]\n",
        "    enhanced_audio = cleaned_waveforms[i]\n",
        "\n",
        "    if enhanced_audio.shape[1] > orig_len:\n",
        "        enhanced_audio = enhanced_audio[:,:orig_len]\n",
        "        noisy_audio = noisy_audio[:,:orig_len]\n",
        "        clean_audio = clean_audio[:,:orig_len]\n",
        "\n",
        "    snr_vals[i] = SNR(enhanced_audio, clean_audio)\n",
        "    pesq_vals[i] = pesq( 16000, enhanced_audio.squeeze().numpy(), clean_audio.squeeze().numpy(), \"wb\")\n",
        "\n",
        "    snr_vals_noisy[i] = SNR(noisy_audio, clean_audio)\n",
        "    pesq_vals_noisy[i] = pesq( 16000, noisy_audio.squeeze().numpy(), clean_audio.squeeze().numpy(), \"wb\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xUkP-YBVe1_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute average and best SNR and PESQ improvements"
      ],
      "metadata": {
        "id": "cyg1ZFIkZQKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SNR_imp = snr_vals - snr_vals_noisy\n",
        "PESQ_imp = pesq_vals - pesq_vals_noisy\n",
        "\n",
        "avg_SNR_imp = SNR_imp.sum() / len(SNR_imp)\n",
        "avg_PESQ_imp = PESQ_imp.sum() / len(PESQ_imp)\n",
        "\n",
        "peak_SNR_imp = SNR_imp.max()\n",
        "peak_snr_ind = SNR_imp.argmax()\n",
        "peak_PESQ_imp = PESQ_imp.max()\n",
        "peak_PESQ_ind = PESQ_imp.argmax()\n",
        "\n",
        "print(\"Average SNR Improvement = \", avg_SNR_imp )\n",
        "print(\"Average PESQ Improvement = \", avg_PESQ_imp )\n",
        "\n",
        "print(\"Best SNR Improvement = \", peak_SNR_imp )\n",
        "print(\"Best PESQ Improvment = \", peak_PESQ_imp )\n",
        "\n",
        "\n",
        "fs = 16000\n",
        "\n",
        "play_audio(test_set[303][0],fs)\n",
        "play_audio(cleaned_waveforms[303],fs)\n",
        "play_audio(test_set[303][1][2],fs)"
      ],
      "metadata": {
        "id": "cIfbEp1hjocU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
